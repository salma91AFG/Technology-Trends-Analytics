ğŸ“˜ Collecte de donnÃ©es via API (Partie 1)

ğŸ“‚ 1_api_data_collection/

Collecte de donnÃ©es dâ€™emploi via une API Flask simulÃ©e

Cette section met en place une API locale utilisÃ©e pour exposer des offres dâ€™emploi et alimenter la premiÃ¨re Ã©tape du pipeline analytique.
Lâ€™objectif est de reproduire un fonctionnement rÃ©aliste oÃ¹ un service backend sert de source de donnÃ©es structurÃ©es.

ğŸ” 1. Vue dâ€™ensemble

Lâ€™API Flask sert de couche dâ€™accÃ¨s aux donnÃ©es et expose un dataset JSON contenant des annonces dâ€™emploi.
Le notebook associÃ© interroge cette API, filtre les rÃ©sultats selon diffÃ©rents critÃ¨res, agrÃ¨ge les donnÃ©es pertinentes et exporte les rÃ©sultats dans un format exploitable.

Ce module couvre :

la consommation dâ€™un endpoint API

lâ€™utilisation de paramÃ¨tres de requÃªte (query string)

la rÃ©cupÃ©ration de donnÃ©es JSON

lâ€™agrÃ©gation (par localisation et par technologie)

la gÃ©nÃ©ration dâ€™un fichier Excel pour les Ã©tapes suivantes

ğŸ§± 2. Architecture du module
1_api_data_collection/
â”‚
â”œâ”€â”€ API_notebook.ipynb      # Collecte, filtrage, agrÃ©gation et export
â”œâ”€â”€ Jobs_API.ipynb          # API Flask exÃ©cutÃ©e localement
â”œâ”€â”€ jobs.json               # Dataset source exploitÃ© par lâ€™API
â””â”€â”€ job-postings.xlsx       # RÃ©sultats gÃ©nÃ©rÃ©s aprÃ¨s exÃ©cution

âš™ï¸ 3. API Flask : rÃ´le et fonctionnement

Lâ€™API joue le rÃ´le dâ€™un micro-service simple capable dâ€™exposer et de filtrer des offres dâ€™emploi.

Elle :

charge un dataset local (jobs.json)

expose des endpoints pour rÃ©cupÃ©rer les donnÃ©es brutes

applique un filtrage via expressions rÃ©guliÃ¨res selon les paramÃ¨tres fournis

Endpoints principaux
MÃ©thode	Endpoint	Description
GET	/data/all	Retourne toutes les offres dâ€™emploi
GET	/data?...	Filtre selon un ou plusieurs critÃ¨res
ParamÃ¨tres disponibles

Job Title

Key Skills

Location

Role Category

Industry

Role

Exemples de valeurs filtrables :
Python, C++, SQL Server, Seattle, New York, etc.

â–¶ï¸ 4. DÃ©marrage de lâ€™API

Pour lancer le serveur Flask :

Ouvrir Jobs_API.ipynb

ExÃ©cuter toutes les cellules

Le serveur devient accessible sur :

http://127.0.0.1:5000/data


Tant que le notebook reste actif, les autres scripts peuvent interroger lâ€™API via requests.

ğŸ§® 5. Collecte et consolidation

Le notebook API_notebook.ipynb automatise la collecte :

rÃ©cupÃ©ration des offres selon diffÃ©rentes localisations

filtrage selon les technologies recherchÃ©es

comptage du nombre dâ€™annonces pour chaque critÃ¨re

crÃ©ation dâ€™un fichier Excel contenant deux onglets :

jobs locations â†’ Nombre dâ€™offres par ville

langages jobs â†’ Nombre dâ€™offres par technologie

Ce processus fournit un jeu de donnÃ©es structurÃ© et directement exploitable pour les Ã©tapes dâ€™analyse.

ğŸ“¦ 6. RÃ©sultats gÃ©nÃ©rÃ©s

Ã€ lâ€™issue de lâ€™exÃ©cution :

un fichier job-postings.xlsx est gÃ©nÃ©rÃ©

les donnÃ©es agrÃ©gÃ©es peuvent Ãªtre rÃ©utilisÃ©es dans les Ã©tapes suivantes (web scraping, analyse exploratoire, visualisation)

ğŸ§¾ 7. Positionnement dans le pipeline

Cette partie reprÃ©sente lâ€™entrÃ©e du projet.
Elle fournit une source contrÃ´lÃ©e, normalisÃ©e et reproductible, servant de base aux autres modules :

Web Scraping

Analyse exploratoire (EDA)

ModÃ©lisation

Dashboard final
